# Half Semester Project
## Data Science Fundamentals

### Exercise 2:
1. Which packages are available for ML? Describe the pros and cons and document the availability.
   
   There are numerous machine learning packages available, some of the most popular ones include:
   - Scikit-learn: Pros - Easy to use, well-documented, extensive community support. Cons - Limited scalability.
   - TensorFlow: Pros - Excellent for deep learning, good for production-level deployment. Cons - Steeper learning curve.
   - PyTorch: Pros - Dynamic computation graph, good for research and prototyping. Cons - Slightly less mature ecosystem compared to TensorFlow.
   - Keras: Pros - High-level API, easy to use, good for beginners. Cons - May lack flexibility for advanced users.
   - XGBoost/LightGBM/CatBoost: Pros - State-of-the-art gradient boosting implementations. Cons - May require tuning for optimal performance.
   Documentation and availability can be found on the respective websites and GitHub repositories for each package.

2. What is Chembl? How do you access it?
   ChEMBL is a large-scale bioactivity database that provides information on the properties of drugs and drug-like small molecules. It contains data on compounds, their targets, and the biological activity of these compounds. You can access it through the ChEMBL website (https://www.ebi.ac.uk/chembl/) or programmatically via its API.

3. What is machine learning, and how does it differ from traditional programming?
   Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. In traditional programming, rules and instructions are explicitly provided by the programmer to solve a specific task. In contrast, in machine learning, algorithms learn patterns and relationships from data to make predictions or decisions.

4. What are the key concepts and techniques in machine learning?
   Key concepts and techniques include supervised learning, unsupervised learning, reinforcement learning, neural networks, decision trees, support vector machines, clustering, dimensionality reduction, regularization, cross-validation, and model evaluation metrics.

5. What are the different types of machine learning algorithms?
   Machine learning algorithms can be categorized into supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, and self-supervised learning.

6. What are the common applications of machine learning?
   Common applications include image recognition, natural language processing, recommendation systems, predictive analytics, fraud detection, healthcare diagnostics, autonomous vehicles, and robotics.

7. How do you evaluate the performance of a machine learning model?
   Model performance can be evaluated using various metrics such as accuracy, precision, recall, F1-score, ROC curve, AUC-ROC, mean squared error, mean absolute error, etc. The choice of metric depends on the specific problem and the nature of the data.

8. How do you prepare data for use in a machine learning model?
   Data preparation involves tasks such as data cleaning, data transformation, feature selection, feature scaling, handling missing values, encoding categorical variables, and splitting the data into training and testing sets.

9. What are some common challenges in machine learning, and how can they be addressed?
   Common challenges include overfitting, underfitting, data scarcity, imbalanced data, noisy data, feature engineering, model interpretability, and scalability. These challenges can be addressed through techniques such as regularization, cross-validation, data augmentation, ensemble methods, and careful feature selection.

10. What are some resources and tools available to help you learn and practice machine learning?
    Resources include online courses (e.g., Coursera, Udacity, edX), books (e.g., "Introduction to Statistical Learning" by James et al., "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by GÃ©ron), MOOCs (e.g., fast.ai), and tutorials (e.g., Towards Data Science, Kaggle). Tools include Jupyter notebooks, Kaggle kernels, TensorFlow Playground, Google Colab, and various machine learning libraries mentioned earlier.

### Exercise 3: Machine Learning Tutorial
1. What is in the training set, how big is it?
   As the data set we use the kinase.csv file provided by the course coordinators. This is split up into 70% training and 30% test data. Resulting in a training set containing 125878 rows of 3 columns. Its job is to provide data which the model can use to establish a connection between a certain MAAC fingerprint and the IC50 values.

2. What modifications do you need to do to the data set to perform the tutorial.
   The dataset we received contains the IC50 of the compound. Since the online template works with the pIC values (negative decadig logarythm) those had to be calculated and added first. 
   Futher the strings in "smiles" needs to be converted to numerical data to apply a neural network. The MACCS fingerprint is saved as "fingerprints_df".

3. What is a test set? Any other types of set?
   The test set is used to evaluate how well the model's predicted values fit the actual ones, after it has been trained. Therefore the model firstly only receives the MAACs of the compunds and predictes a value, which is subsequently compared to the real value.
   Next to the previously mentioned Training set, according to ChatGPT there exists also the so called Validation Set: It is often used during the training process to tune hyperparameters and assess the model's performance on data that it hasn't been directly trained on. This helps to prevent overfitting to the training data and to select the best-performing model among different hyperparameter settings.

4. Before starting describe with 1-2 sentences, in your own words, what is done in each of the cells
   [1] First of all import all the necessary libraries needed, which can be copied by the tutorial
   [2] Set and ensure the notebooks path is correctly set
   [3] Load the needed data to work on it
   [4] Before proceeding with the data, it's essential to chekc the dimensions and also the presence of any missing values within the dataset. This will also help to understand the datesets structe and quality.
   [5] Taking a quick look at the first row of the dataset, to ensure everything worked as wanted
   [6] Determine and retain necessary columns for the analysis.
   [7] Defined a function "smiles_to_fp" which generates MACCS fingerprints from "smiles"
   [8] Execute "smiles_to_fp" to convert all SMILES strings to MACCS fingerprints. This step is crucial for preparing the molecular data for the subsequent modeling steps. 
   [9] Split the data into 70% training and 30% test data.
   [10] Define neural network with the number of neurons in hidden layers and the activation function as arguments
   [11] Define neural network parameters, which will be dicided by the parameters on the headline
   [12] Plot the respective losses, which are generated.
   [13] Save the model, whichs batch size gives the best performance for the model choosen.
   [14] Evaluate the model on the test data, to give you a first overall look at the data.
   [15] Before the visualizations, the IC50 values on the test data has to be predicted
   [16] Visualize the predictions by plotting the predicted against the true IC50 values on the test set. This gives you all the needed information for this dataset.

### Exercise 5:
   1. UBELIX is an HPC cluster that currently consists of about 320 compute nodes featuring round about 12k CPU cores and 160 GPUs and a software defined storage infrastructure providing about 3.5 PB of disk storage net. 
      UBELIX is a heterogenous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. 
   2. You have to request it on from the Universitiy and then get access from the Unibern. Getting access takes one day and you have to use it on your powershell. You need to be on the network of the University. 
   3. First you have to login to the Cluster. For this you have to use a secure shell (SSH). To submit a job you first have to ensure that your files are in the correct directory. IF you need to copy files between your local computer and the cluster, you can use the secure copy comand scp. And to copy a file from the cluster to your local computer you have to run a UNIX-like OS. For a job script containing instructions for the scheduler. You have to define the resource requirements and specify the tasks to be executed. The submit your job to the scheduler using the sbatch command.
   4. Only members of the Universitty have access and the access is primarily granted for researc haligned with the universitys interests. 
   5. UBELIX provides an array of computational resources, which are already listet on 1.

